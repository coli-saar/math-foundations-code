{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters our neural network are two weight matrices, W and V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 1.]]\n",
      "[[ 1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[1,2],[3,1]], dtype=float) # shape (2,2)\n",
    "V = np.array([[1,-1]], dtype=float) # shape (1,2)\n",
    "\n",
    "print(W)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks reads an input xx from R2 and predicts an output y. For simplicity, we let y be a single float, i.e. a vector from R1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sigmoid = logistic function.\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Forward application of the neural network; returns the prediction.\n",
    "def forward(xx, W, V):\n",
    "    hh = W @ xx         # shape (2,)\n",
    "    aa = sigmoid(hh)    # shape (2,)\n",
    "    y = V @ aa          # shape (1,)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward evaluation of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some training data for the neural network. For each xx, it is supposed to predict the corresponding gold y. So for instance, for the input [2,1], the NN is supposed to predict 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.array([2,1])\n",
    "xx2 = np.array([0,1])\n",
    "\n",
    "yhat = 1\n",
    "yhat2 = -1\n",
    "\n",
    "inputs = [xx, xx2]\n",
    "gold_outputs = np.array([yhat, yhat2])\n",
    "\n",
    "N = len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in reality, the NN makes really crazy predictions. This is because W and V don't have the correct entries to predict the input-output mapping in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.017075158767690946"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = forward(xx, W, V)[0]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantify how much a prediction of the NN differs from the gold output using a loss function. Here we use the squared error loss.\n",
    "\n",
    "The _cost_ of a training instance is the value of the loss function on the NN's prediction for this instance, compared to the gold output in this instance. The overall cost on the whole training set is the sum of costs on the individual instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0344418785823237"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y, yhat):\n",
    "    return (y-yhat)**2\n",
    "\n",
    "loss(y_pred, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3218986168827092"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cost of the second training instance\n",
    "loss(forward(xx2, W, V)[0], yhat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing W and V by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change the values of W and V, the cost will change. Our goal is to find values of W and V that minimize the overall cost, ideally to zero (which would mean that the NN predicts the gold outputs perfectly on all training instances). But doing this by hand is difficult. Let's try a few variants of W and V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.356340495465033"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1 = loss(forward(xx, W, V)[0], yhat)\n",
    "loss2 = loss(forward(xx2, W, V)[0], yhat2)\n",
    "loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.784352555120851"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.array([[1,1]])\n",
    "\n",
    "loss1 = loss(forward(xx, W, V)[0], yhat)\n",
    "loss2 = loss(forward(xx2, W, V)[0], yhat2)\n",
    "loss1 + loss2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.06868611336519"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.array([[0,-1]])\n",
    "\n",
    "loss1 = loss(forward(xx, W, V)[0], yhat)\n",
    "loss2 = loss(forward(xx2, W, V)[0], yhat2)\n",
    "loss1 + loss2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.53772115227894"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.array([[1,0]])\n",
    "\n",
    "loss1 = loss(forward(xx, W, V)[0], yhat)\n",
    "loss2 = loss(forward(xx2, W, V)[0], yhat2)\n",
    "loss1 + loss2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, it is really difficult to find a better V by hand. Let's do it automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropagation algorithm will compute the gradient of the cost with respect to W and V. The gradient represent the direction of steepest ascent, i.e. the changes to W and V that would increase the cost as quickly as possible. By _subtracting_ a multiple of the gradients from W and V, we can minimize the cost. First, let's define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rows x columns Numpy array and fill it with the\n",
    "# values of f(row,column).\n",
    "\n",
    "def make_matrix(rows, columns, f):\n",
    "    values = [[f(i,j) for j in range(columns)] for i in range(rows)]\n",
    "    values = itertools.chain.from_iterable(values)\n",
    "    arr = np.fromiter(values, dtype=float)\n",
    "    return np.reshape(arr, (rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of the sigmoid function.\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a class that computes gradients for each training instance. Afterwards, it will sum the gradients over all training instances. By the sum rule of calculus, this gives us the gradient for the overall cost.\n",
    "\n",
    "We also redefine the \"forward\" function from above so it computes and collects the gradients. Notice that the backpropagation algorithm needs access to the intermediate results of the calculation, so have to \"look inside\" the forward algorithm and can't just look at the output. Real NN libraries such as Pytorch or Tensorflow do this automatically by keeping track of the computation graph, so the code can remain more uncluttered than ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backprop:\n",
    "    def __init__(self, N):\n",
    "        self.gradients = {\"V\": [], \"W\": []}\n",
    "        self.N = N\n",
    "        \n",
    "    def collect(self, xx, hh, aa, y, ygold):\n",
    "        # Jacobi matrices\n",
    "        j_e_y = make_matrix(1, 1, lambda row, col: 2 * (y-ygold))  # (1,1)\n",
    "        j_y_aa = V   # (1,2)\n",
    "        j_aa_hh = np.diag(d_sigmoid(hh)) # (2,2)   \n",
    "\n",
    "        # gradients\n",
    "        grad_e = np.array([[1]])     # shape (1,)\n",
    "        grad_y = j_e_y.T @ grad_e    # shape (1,)\n",
    "        grad_aa = j_y_aa.T @ grad_y  # shape (2,)\n",
    "        grad_hh = j_aa_hh.T @ grad_aa # shape (2,)\n",
    "        grad_V = make_matrix(1, 2, lambda row, col: aa[col] * grad_y[row])\n",
    "        grad_W = make_matrix(2, 2, lambda row, col: xx[col] * grad_hh[row])\n",
    "\n",
    "        self.gradients[\"V\"].append(grad_V)\n",
    "        self.gradients[\"W\"].append(grad_W)\n",
    "        \n",
    "    def get_gradients(self):\n",
    "        total_grad_V = sum(self.gradients[\"V\"])\n",
    "        total_grad_W = sum(self.gradients[\"W\"])\n",
    "        return total_grad_V, total_grad_W\n",
    "    \n",
    "\n",
    "def forward(xx, W, V, ygold=None, accumulate_gradient=None):\n",
    "    hh = W @ xx         # shape (2,)\n",
    "    aa = sigmoid(hh)    # shape (2,)\n",
    "    y = V @ aa          # shape (1,)\n",
    "    \n",
    "    if accumulate_gradient: # an object of class Backprop\n",
    "        accumulate_gradient.collect(xx, hh, aa, y, ygold)\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to perform gradient descent training. For \"num_epochs\" epochs, we iterate over the training data and collect the gradients for each training instance. At the end of the epoch, we sum the gradients up and perform one step of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    global V, W\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        total_cost = 0\n",
    "        backprop = Backprop(N)\n",
    "        \n",
    "        # iterate over all training instances and collect instance-wise gradients\n",
    "        for k in range(N):\n",
    "            y_pred = forward(inputs[k], W, V, ygold=gold_outputs[k], accumulate_gradient=backprop)\n",
    "            cost_here = loss(y_pred, gold_outputs[k])\n",
    "            total_cost += cost_here\n",
    "            \n",
    "        costs.append(total_cost)\n",
    "        \n",
    "        # gradient descent step\n",
    "        total_grad_V, total_grad_W = backprop.get_gradients()        \n",
    "        V += - learning_rate * total_grad_V\n",
    "        W += - learning_rate * total_grad_W\n",
    "        \n",
    "    return costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the actual gradient descent training, for 1000 epochs. You should experiment with different values for the learning rate and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "W = np.array([[1,2],[3,1]], dtype=float) # shape (2,2)\n",
    "V = np.array([[1,-1]], dtype=float) # shape (1,2)\n",
    "\n",
    "# perform GD\n",
    "learning_rate = 0.1\n",
    "costs = train(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot the learning curve, which shows the cost before each training epoch. Notice that we can see that training converged because the curve has flattened out. In practical work with NNs, you should always look at learning curves to see if you trained the NN for enough epochs. You can also diagnose certain problems from the learning curve: e.g. if the cost fluctuates wildly, your learning rate is probably too high. Try this out (e.g. set the LR to 0.8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa81228f610>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xticks(range(0, len(costs), 100))\n",
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the NN actually learned to make the right predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999983542\n",
      "-0.9999999999976827\n"
     ]
    }
   ],
   "source": [
    "print(forward(xx, W, V)[0])\n",
    "print(forward(xx2, W, V)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
